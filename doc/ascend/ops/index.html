<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>ascend.ops API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<link rel="preconnect" href="https://www.google.com">
<script async src="https://cse.google.com/cse.js?cx=017837193012385208679:pey8ky8gdqw"></script>
<style>
.gsc-control-cse {padding:0 !important;margin-top:1em}
body.gsc-overflow-hidden #sidebar {overflow: visible;}
</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
<style>.homelink{display:block;font-size:2em;font-weight:bold;color:#555;padding-bottom:.5em;border-bottom:1px solid silver}.homelink:hover{color:inherit}.homelink img{max-width:20%;max-height:5em;margin:auto;margin-bottom:.3em}</style>
<link rel="canonical" href="https://pdoc3.github.io/pdoc/doc/ascend/ops/">
<link rel="icon" href="https://gitee.com/ascend-fae/ascendfly/blob/master/doc/logo/logo.png">
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ascend.ops</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/__init__.py#L0-L6" class="git-link">Browse git</a>
</summary>
<pre><code class="python">from .blas import Matmul, Vmul
from .op import Cast, ArgMax, Transpose, Permute

__all__ = [
    &#39;Matmul&#39;, &#39;Vmul&#39;, &#39;Cast&#39;, &#39;ArgMax&#39;, &#39;Transpose&#39;,
    &#39;Permute&#39;,
]</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="ascend.ops.blas" href="blas.html">ascend.ops.blas</a></code></dt>
<dd>
<div class="desc"><p>Copyright 2020 Huawei Technologies Co., Ltd
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in …</p></div>
</dd>
<dt><code class="name"><a title="ascend.ops.csc" href="csc.html">ascend.ops.csc</a></code></dt>
<dd>
<div class="desc"><p>Copyright 2020 Huawei Technologies Co., Ltd
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in …</p></div>
</dd>
<dt><code class="name"><a title="ascend.ops.math" href="math.html">ascend.ops.math</a></code></dt>
<dd>
<div class="desc"><p>Copyright 2020 Huawei Technologies Co., Ltd
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in …</p></div>
</dd>
<dt><code class="name"><a title="ascend.ops.op" href="op.html">ascend.ops.op</a></code></dt>
<dd>
<div class="desc"><p>Copyright 2020 Huawei Technologies Co., Ltd
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in …</p></div>
</dd>
<dt><code class="name"><a title="ascend.ops.util" href="util.html">ascend.ops.util</a></code></dt>
<dd>
<div class="desc"><p>Copyright 2020 Huawei Technologies Co., Ltd
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in …</p></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ascend.ops.ArgMax"><code class="flex name class">
<span>class <span class="ident">ArgMax</span></span>
<span>(</span><span>input, axis=0, context=None, stream=None)</span>
</code></dt>
<dd>
<div class="desc"><p>define a ArgMax operator.</p>
<h2 id="args">Args</h2>
<dl>
<dt>input
: input tensor (AscendArray)</dt>
<dt>size
: output data size</dt>
<dt><strong><code>context</code></strong></dt>
<dd>input context, optinal</dd>
</dl>
<p>stream : input stream, optinal
function:
run
: do argmax
out
: return output result</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L106-L203" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class ArgMax():
    &#34;&#34;&#34; define a ArgMax operator.

    Args:
        input  : input tensor (AscendArray)
        size   : output data size
        context: input context, optinal
        stream : input stream, optinal

    function:
        run    : do argmax
        out    : return output result
    &#34;&#34;&#34;
    def __init__(self, input, axis=0, context=None, stream=None):
        if not isinstance(input, AscendArray):
            raise TypeError(f&#34;Input tensor expects a AscendArray, but got {type(input)}.&#34;)

        if context and not isinstance(context, int):
            raise TypeError(f&#34;Input context expects an int, but got {type(context)}.&#34;)

        if stream and not isinstance(stream, int):
            raise TypeError(f&#34;Input stream expects an int, but got {type(stream)}.&#34;)

        if axis &gt;= input.ndim:
            raise ValueError(f&#34;Input axis should in range [0, {input.ndim}).&#34;)

        # assign self value
        self.input = input
        self.context = context
        self.stream = stream if stream else create_stream(context)
        self.created = stream is None

        # set op model dir
        OpSet()
        self.__pre_set()

        # create output array to save result
        shape = input.shape[:axis] + input.shape[axis + 1:]
        self._dim = AscendArray.clone(np.array(axis, dtype=np.int32))
        self._out = AscendArray(shape, dtype=np.int32)

        self.tensor_in = TensorDesc(input)
        self.tensor_dim = TensorDesc(self._dim)
        self.tensor_out = TensorDesc(self._out)

        # do cast operator
        self.run()

    def __pre_set(self):
        &#34;&#34;&#34; set op name and attribute.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        self.op_name = &#34;ArgMaxV2&#34;
        self.op_attr = acl.op.create_attr()

    def run(self):
        &#34;&#34;&#34; run op.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        # do op excute
        ret = acl.op.execute(self.op_name,
                             [self.tensor_in.desc, self.tensor_dim.desc],
                             [self.tensor_in.buff, self.tensor_dim.buff],
                             [self.tensor_out.desc],
                             [self.tensor_out.buff],
                             self.op_attr,
                             self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to excute op {self.op_name}, return {ret}.&#34;)

        # do synchronize stream 
        ret = acl.rt.synchronize_stream(self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to synchronize stream in excute op, return {ret}.&#34;)
    
    @property
    def data(self):
        return self._out

    def __del__(self):
        if hasattr(self, &#39;_out&#39;):
            del self._out
        if hasattr(self, &#39;tensor_out&#39;):
            del self.tensor_out
        if hasattr(self, &#39;tensor_in&#39;):
            del self.tensor_in

        if self.created:
            ret = acl.rt.destroy_stream(self.stream)
            assert ret == ACL_SUCCESS, f&#34;destroy stream failed, return {ret}.&#34;</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="ascend.ops.ArgMax.data"><code class="name">var <span class="ident">data</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L189-L191" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@property
def data(self):
    return self._out</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ascend.ops.ArgMax.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>run op.</p>
<h2 id="args">Args</h2>
<p>None</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L165-L187" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34; run op.
    Args:
        None

    Returns:
        None
    &#34;&#34;&#34;
    # do op excute
    ret = acl.op.execute(self.op_name,
                         [self.tensor_in.desc, self.tensor_dim.desc],
                         [self.tensor_in.buff, self.tensor_dim.buff],
                         [self.tensor_out.desc],
                         [self.tensor_out.buff],
                         self.op_attr,
                         self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to excute op {self.op_name}, return {ret}.&#34;)

    # do synchronize stream 
    ret = acl.rt.synchronize_stream(self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to synchronize stream in excute op, return {ret}.&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ascend.ops.Cast"><code class="flex name class">
<span>class <span class="ident">Cast</span></span>
<span>(</span><span>input, dtype=dtype('float16'), context=None, stream=None)</span>
</code></dt>
<dd>
<div class="desc"><p>define a Cast operator object to release dtype convert. support translate types:
float -&gt; float16, float16 -&gt; float, uint8 -&gt; float16, float16 -&gt; uint8
uint8 -&gt; float32, float32 -&gt; uint8, float16 -&gt; int32, int32 -&gt; float16
</p>
<h2 id="args">Args</h2>
<dl>
<dt>input
: input tensor (AscendArray)</dt>
<dt>dtype
: the converted data type of input.</dt>
<dt><strong><code>context</code></strong></dt>
<dd>input context, optional</dd>
</dl>
<p>stream : input stream, optional
function:
run
: do compute matmul
out
: return output result</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L25-L103" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Cast():
    &#34;&#34;&#34; define a Cast operator object to release dtype convert. support translate types:
        float -&gt; float16, float16 -&gt; float, uint8 -&gt; float16, float16 -&gt; uint8
        uint8 -&gt; float32, float32 -&gt; uint8, float16 -&gt; int32, int32 -&gt; float16  

    Args:
        input  : input tensor (AscendArray)
        dtype  : the converted data type of input.
        context: input context, optional
        stream : input stream, optional

    function:
        run             : do compute matmul
        out             : return output result
    &#34;&#34;&#34;
    def __init__(self, input, dtype=np.dtype(&#39;float16&#39;), context=None, stream=None):
        if not isinstance(input, AscendArray):
            raise TypeError(f&#34;Input tensor expects a AscendArray, but got {type(input)}.&#34;)

        if context and not isinstance(context, int):
            raise TypeError(f&#34;Input context expects an int, but got {type(context)}.&#34;)

        if stream and not isinstance(stream, int):
            raise TypeError(f&#34;Input stream expects an int, but got {type(stream)}.&#34;)

        # assign self value
        self.input = input
        self.context = context
        self.stream = stream if stream else create_stream(context)
        self.created = stream is None

        OpSet()
         # create output array to save result
        self.output = AscendArray(input.shape, dtype=dtype, format=&#39;ND&#39;)
    
        self.tensor_in = TensorDesc(input)
        self.tensor_out = TensorDesc(self.output)

        # do cast operator
        self.run()
        
    def run(self):
        &#34;&#34;&#34; run op.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        # do op cast
        ret = acl.op.cast(self.tensor_in.desc,
                          self.tensor_in.buff,
                          self.tensor_out.desc,
                          self.tensor_out.buff,
                          0,
                          self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to do op cast, return {ret}.&#34;)

        # do synchronize stream 
        ret = acl.rt.synchronize_stream(self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to synchronize stream in running blas gemm_ex, return {ret}.&#34;)
    
    @property
    def data(self):
        return self.output

    def __del__(self):
        if hasattr(self, &#39;output&#39;):
            del self.output
        if hasattr(self, &#39;tensor_out&#39;):
            del self.tensor_out
        if hasattr(self, &#39;tensor_in&#39;):
            del self.tensor_in

        if self.created:
            ret = acl.rt.destroy_stream(self.stream)
            assert ret == ACL_SUCCESS, f&#34;destroy stream failed, return {ret}.&#34;</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="ascend.ops.Cast.data"><code class="name">var <span class="ident">data</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L89-L91" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@property
def data(self):
    return self.output</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ascend.ops.Cast.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>run op.</p>
<h2 id="args">Args</h2>
<p>None</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L66-L87" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34; run op.
    Args:
        None

    Returns:
        None
    &#34;&#34;&#34;
    # do op cast
    ret = acl.op.cast(self.tensor_in.desc,
                      self.tensor_in.buff,
                      self.tensor_out.desc,
                      self.tensor_out.buff,
                      0,
                      self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to do op cast, return {ret}.&#34;)

    # do synchronize stream 
    ret = acl.rt.synchronize_stream(self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to synchronize stream in running blas gemm_ex, return {ret}.&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ascend.ops.Matmul"><code class="flex name class">
<span>class <span class="ident">Matmul</span></span>
<span>(</span><span>mat_a, mat_b, mat_c, alpha=1.0, beta=0.0, highprec=True, context=None)</span>
</code></dt>
<dd>
<div class="desc"><p>define a Matmul object, release the function .</p>
<p>Attributes::
context: the output image bind with an AscendArray object, image.shape(tupe(h, w, c))
mat_a
:
mat_b
:
mat_c
:</p>
<h2 id="methods">Methods</h2>
<p>__pre_compute : prepare input data
run
: do compute matmul
out
: return output result</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/blas.py#L38-L190" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Matmul():
    &#34;&#34;&#34; define a Matmul object, release the function .

    Attributes::
        context: the output image bind with an AscendArray object, image.shape(tupe(h, w, c))
        mat_a  : 
        mat_b  : 
        mat_c  :

    Methods:
        __pre_compute : prepare input data
        run           : do compute matmul
        out           : return output result
    &#34;&#34;&#34;
    def __init__(self, mat_a, mat_b, mat_c, alpha=1.0, beta=0.0, highprec=True, context=None):
        if context and not isinstance(context, int):
            raise TypeError(f&#34;Input context expects an int, but got {type(context)}.&#34;)

        if not isinstance(mat_a, AscendArray):
            raise TypeError(f&#34;Input mat_a expects an AscendArray, but got {type(mat_a)}.&#34;)

        if not isinstance(mat_b, AscendArray):
            raise TypeError(f&#34;Input mat_a expects an AscendArray, but got {type(mat_b)}.&#34;)

        if not isinstance(mat_c, AscendArray):
            raise TypeError(f&#34;Input mat_a expects an AscendArray, but got {type(mat_c)}.&#34;)

        assert mat_a.format == mat_b.format, f&#34;Input mat_a and mat_b expects same format.&#34;

        # assign self value
        self.context = context
        self.stream = create_stream(context)
        self.highprec = 1 if highprec else 0

        pdb.set_trace()
        # set op model dir
        ret = acl.op.set_model_dir(&#34;./om&#34;)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;op set model dir failed, return {ret}.&#34;)
        
        # calculate m, n, k and trans alpha and beta to np.ndarray
        self.__pre_compute(mat_a, mat_b, mat_c, alpha, beta)

        # do blas gemm_ex and synchronize stream
        self.run()

        # free input data memory
        self.free_ab()


    def __pre_compute(self, mat_a, mat_b, mat_c, alpha, beta):
        &#34;&#34;&#34; calculate m, n, k and copy alpha/beta to device.
        Args:
            mat_a : (AscendArray) matrix A
            mat_b : (AscendArray) matrix B
            mat_c : (AscendArray) matrix C
            alpha : (float value)
            beta  : (float value)

        Returns:
            None
        &#34;&#34;&#34;
        if mat_a.format in [
            PIXEL_FORMAT_YUV_SEMIPLANAR_420,
            PIXEL_FORMAT_YVU_SEMIPLANAR_420
            ]:
            self.m = mat_a.shape[0] * 2 // 3
            self.n = mat_b.shape[-1]
            self.k = mat_a.shape[1]
        elif mat_a.format in [
            PIXEL_FORMAT_RGB_888,
            PIXEL_FORMAT_BGR_888
            ]:
            self.m = mat_a.shape[0]
            self.n = mat_b.shape[-1]
            self.k = mat_a.shape[1]
        elif mat_a.format == &#39;NCHW&#39;:
            self.m = mat_a.shape[2]
            self.n = mat_b.shape[-1]
            self.k = mat_a.shape[-1]
        elif mat_a.format == &#39;NHWC&#39;:
            self.m = mat_a.shape[1]
            self.n = mat_b.shape[-1]
            self.k = mat_a.shape[2]
        else:
            raise ValueError(f&#34;Input data format not support.&#34;)

        alpha = np.array([alpha]).astype(mat_a.dtype)
        beta  = np.array([beta]).astype(mat_a.dtype)
        self.alpha = AscendArray.clone(alpha)
        self.beta = AscendArray.clone(beta)

    def run(self):
        &#34;&#34;&#34; run op.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        a_type = type_map(self.mat_a.dtype)
        b_type = type_map(self.mat_a.dtype)
        c_type = type_map(self.mat_a.dtype)
        # do gemm asyncronize
        ret = acl.blas.gemm_ex(ACL_TRANS_N, ACL_TRANS_N, ACL_TRANS_N, 
                                self.m, self.n, self.k, 
                                self.alpha.ascend_data,
                                self.mat_a.ascend_data, self.k, a_type, 
                                self.mat_b.ascend_data, self.n, b_type, 
                                self.beta.ascend_data,
                                self.mat_c.ascend_data, self.n, c_type, 
                                self.high_prec, 
                                self.stream) 
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to do blas gemm_ex, return {ret}.&#34;)

        # do synchronize stream 
        ret = acl.rt.synchronize_stream(self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to synchronize stream in running blas gemm_ex, return {ret}.&#34;)
    
    @property
    def out(self):
        &#34;&#34;&#34; run op.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        return self.mat_c

    def free_ab(self):
        &#34;&#34;&#34; free alpha and beta data memory
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        if hasattr(self, &#39;alpha&#39;):
            del self.alpha

        if hasattr(self, &#39;beta&#39;):
            del self.beta

    def __del__(self):
        if hasattr(self, &#39;stream&#39;):
            ret = acl.rt.destroy_stream(self.stream)
            assert ret == ACL_SUCCESS, f&#34;destroy stream failed, return {ret}.&#34;
        
        # free alpha and beta data memory
        self.free_ab()</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="ascend.ops.Matmul.out"><code class="name">var <span class="ident">out</span></code></dt>
<dd>
<div class="desc"><p>run op.</p>
<h2 id="args">Args</h2>
<p>None</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/blas.py#L159-L168" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@property
def out(self):
    &#34;&#34;&#34; run op.
    Args:
        None

    Returns:
        None
    &#34;&#34;&#34;
    return self.mat_c</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ascend.ops.Matmul.free_ab"><code class="name flex">
<span>def <span class="ident">free_ab</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>free alpha and beta data memory</p>
<h2 id="args">Args</h2>
<p>None</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/blas.py#L170-L182" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def free_ab(self):
    &#34;&#34;&#34; free alpha and beta data memory
    Args:
        None

    Returns:
        None
    &#34;&#34;&#34;
    if hasattr(self, &#39;alpha&#39;):
        del self.alpha

    if hasattr(self, &#39;beta&#39;):
        del self.beta</code></pre>
</details>
</dd>
<dt id="ascend.ops.Matmul.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>run op.</p>
<h2 id="args">Args</h2>
<p>None</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/blas.py#L130-L157" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34; run op.
    Args:
        None

    Returns:
        None
    &#34;&#34;&#34;
    a_type = type_map(self.mat_a.dtype)
    b_type = type_map(self.mat_a.dtype)
    c_type = type_map(self.mat_a.dtype)
    # do gemm asyncronize
    ret = acl.blas.gemm_ex(ACL_TRANS_N, ACL_TRANS_N, ACL_TRANS_N, 
                            self.m, self.n, self.k, 
                            self.alpha.ascend_data,
                            self.mat_a.ascend_data, self.k, a_type, 
                            self.mat_b.ascend_data, self.n, b_type, 
                            self.beta.ascend_data,
                            self.mat_c.ascend_data, self.n, c_type, 
                            self.high_prec, 
                            self.stream) 
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to do blas gemm_ex, return {ret}.&#34;)

    # do synchronize stream 
    ret = acl.rt.synchronize_stream(self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to synchronize stream in running blas gemm_ex, return {ret}.&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ascend.ops.Permute"><code class="flex name class">
<span>class <span class="ident">Permute</span></span>
<span>(</span><span>input, axes=(0, 2, 3, 1), context=None, stream=None)</span>
</code></dt>
<dd>
<div class="desc"><p>define a Permute operator object to permute the dimensions. </p>
<p>Attributes::
input
: input tensor (AscendArray)
axes
: Permutes the dimensions.
context: input context, optional
stream : input stream, optional</p>
<h2 id="methods">Methods</h2>
<p>run
: do permute
out
: return output result</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L305-L406" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Permute():
    &#34;&#34;&#34; define a Permute operator object to permute the dimensions. 

    Attributes::
        input  : input tensor (AscendArray)
        axes   : Permutes the dimensions.
        context: input context, optional
        stream : input stream, optional

    Methods:
        run    : do permute
        out    : return output result
    &#34;&#34;&#34;
    def __init__(self, input, axes=(0, 2, 3, 1), context=None, stream=None):
        if not isinstance(input, AscendArray):
            raise TypeError(f&#34;Input tensor expects a AscendArray, but got {type(input)}.&#34;)
        
        if not isinstance(axes, (tuple, list)):
            raise TypeError(f&#34;Input axes expects a tuple or list, but got {type(axes)}.&#34;)

        if context and not isinstance(context, int):
            raise TypeError(f&#34;Input context expects an int, but got {type(context)}.&#34;)

        if stream and not isinstance(stream, int):
            raise TypeError(f&#34;Input stream expects an int, but got {type(stream)}.&#34;)

        if tuple(axes) not in [(0, 2, 3, 1), (0, 3, 1, 2)]:
            raise ValueError(f&#34;Input axis only support (0, 2, 3, 1) or (0, 2, 3, 1).&#34;)
            
        # assign self value
        self._axes = axes
        self.context = context
        self.stream = stream if stream else create_stream(context)
        self.created = stream is None

        OpSet()
        self.__pre_set()

        # create output array to save result
        out_shape = tuple([input.shape[i] for i in axes])
        self.output = AscendArray(out_shape, dtype=input.dtype, format=&#39;ND&#39;)
        self.tensor_in = TensorDesc(input)
        self.tensor_out = TensorDesc(self.output)

        # do transpose
        self.run()


    def __pre_set(self):
        &#34;&#34;&#34; set op name and attribute.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        self.op_name = &#34;Permute&#34;
        self.op_attr = acl.op.create_attr()
        ret = acl.op.set_attr_list_int(self.op_attr, &#39;order&#39;, np.array(self._axes, dtype=np.int64))
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Set attr &#39;order&#39; failed, return {ret}.&#34;)


    def run(self):
        &#34;&#34;&#34; run op.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        # do op excute
        ret = acl.op.execute(self.op_name,
                             [self.tensor_in.desc],
                             [self.tensor_in.buff],
                             [self.tensor_out.desc],
                             [self.tensor_out.buff],
                             self.op_attr,
                             self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to excute op {self.op_name}, return {ret}.&#34;)

        # do synchronize stream 
        ret = acl.rt.synchronize_stream(self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to synchronize stream in excute op, return {ret}.&#34;)
    
    @property
    def data(self):
        return self.output

    def __del__(self):
        if hasattr(self, &#39;output&#39;):
            del self.output
        if hasattr(self, &#39;tensor_out&#39;):
            del self.tensor_out
        if hasattr(self, &#39;tensor_in&#39;):
            del self.tensor_in

        if hasattr(self, &#39;created&#39;) and self.created:
            ret = acl.rt.destroy_stream(self.stream)
            assert ret == ACL_SUCCESS, f&#34;destroy stream failed, return {ret}.&#34;</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="ascend.ops.Permute.data"><code class="name">var <span class="ident">data</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L392-L394" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@property
def data(self):
    return self.output</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ascend.ops.Permute.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>run op.</p>
<h2 id="args">Args</h2>
<p>None</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L368-L390" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34; run op.
    Args:
        None

    Returns:
        None
    &#34;&#34;&#34;
    # do op excute
    ret = acl.op.execute(self.op_name,
                         [self.tensor_in.desc],
                         [self.tensor_in.buff],
                         [self.tensor_out.desc],
                         [self.tensor_out.buff],
                         self.op_attr,
                         self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to excute op {self.op_name}, return {ret}.&#34;)

    # do synchronize stream 
    ret = acl.rt.synchronize_stream(self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to synchronize stream in excute op, return {ret}.&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ascend.ops.Transpose"><code class="flex name class">
<span>class <span class="ident">Transpose</span></span>
<span>(</span><span>input, perm=[0, 1, 2, 3], context=None, stream=None)</span>
</code></dt>
<dd>
<div class="desc"><p>define a Transpose operator object to release Transpose. Permutes the dimensions according to perm.
The returned tensor's dimension i will correspond to the input dimension perm[i] </p>
<h2 id="args">Args</h2>
<dl>
<dt>input
: input tensor (AscendArray)</dt>
<dt>perm
: Permutes the dimensions.</dt>
<dt><strong><code>context</code></strong></dt>
<dd>input context, optional</dd>
</dl>
<p>stream : input stream, optional</p>
<h2 id="methods">Methods</h2>
<p>run
: do permute
out
: return output result</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L207-L300" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Transpose():
    &#34;&#34;&#34; define a Transpose operator object to release Transpose. Permutes the dimensions according to perm. 
        The returned tensor&#39;s dimension i will correspond to the input dimension perm[i] 

    Args:
        input  : input tensor (AscendArray)
        perm   : Permutes the dimensions.
        context: input context, optional
        stream : input stream, optional

    Methods:
        run    : do permute
        out    : return output result
    &#34;&#34;&#34;
    def __init__(self, input, perm=[0, 1, 2, 3], context=None, stream=None):
        if not isinstance(input, AscendArray):
            raise TypeError(f&#34;Input tensor expects a AscendArray, but got {type(input)}.&#34;)

        if context and not isinstance(context, int):
            raise TypeError(f&#34;Input context expects an int, but got {type(context)}.&#34;)

        if stream and not isinstance(stream, int):
            raise TypeError(f&#34;Input stream expects an int, but got {type(stream)}.&#34;)

        # assign self value
        self.context = context
        self.stream = stream if stream else create_stream(context)
        self.created = stream is None

        OpSet()
        self.__pre_set()

        # create output array to save result
        _perm = AscendArray.clone(np.array(perm, dtype=&#39;int32&#39;))
        self.output = AscendArray(input.shape, dtype=input.dtype, format=&#39;ND&#39;)
        self.tensor_in1 = TensorDesc(input)
        self.tensor_in2 = TensorDesc(_perm)
        self.tensor_out = TensorDesc(self.output)

        # do transpose
        self.run()


    def __pre_set(self):
        &#34;&#34;&#34; set op name and attribute.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        self.op_name = &#34;Transpose&#34;
        self.op_attr = acl.op.create_attr()


    def run(self):
        &#34;&#34;&#34; run op.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        # do op excute
        ret = acl.op.execute(self.op_name,
                             [self.tensor_in1.desc, self.tensor_in2.desc],
                             [self.tensor_in1.buff, self.tensor_in2.buff],
                             [self.tensor_out.desc],
                             [self.tensor_out.buff],
                             self.op_attr,
                             self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to excute op {self.op_name}, return {ret}.&#34;)

        # do synchronize stream 
        ret = acl.rt.synchronize_stream(self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to synchronize stream in excute op, return {ret}.&#34;)
    
    @property
    def data(self):
        return self.output

    def __del__(self):
        if hasattr(self, &#39;output&#39;):
            del self.output
        if hasattr(self, &#39;tensor_out&#39;):
            del self.tensor_out
        if hasattr(self, &#39;tensor_in&#39;):
            del self.tensor_in

        if self.created:
            ret = acl.rt.destroy_stream(self.stream)
            assert ret == ACL_SUCCESS, f&#34;destroy stream failed, return {ret}.&#34;</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="ascend.ops.Transpose.data"><code class="name">var <span class="ident">data</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L286-L288" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@property
def data(self):
    return self.output</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ascend.ops.Transpose.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>run op.</p>
<h2 id="args">Args</h2>
<p>None</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/op.py#L262-L284" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34; run op.
    Args:
        None

    Returns:
        None
    &#34;&#34;&#34;
    # do op excute
    ret = acl.op.execute(self.op_name,
                         [self.tensor_in1.desc, self.tensor_in2.desc],
                         [self.tensor_in1.buff, self.tensor_in2.buff],
                         [self.tensor_out.desc],
                         [self.tensor_out.buff],
                         self.op_attr,
                         self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to excute op {self.op_name}, return {ret}.&#34;)

    # do synchronize stream 
    ret = acl.rt.synchronize_stream(self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to synchronize stream in excute op, return {ret}.&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="ascend.ops.Vmul"><code class="flex name class">
<span>class <span class="ident">Vmul</span></span>
<span>(</span><span>context, mat_a, vec_x, vec_y, alpha=1.0, beta=0.0, highprec=True)</span>
</code></dt>
<dd>
<div class="desc"><p>define a Matmul object, release the function .</p>
<p>Attributes::
context: the output image bind with an AscendArray object, image.shape(tupe(h, w, c))
mat_a
:
mat_b
:
mat_c
:</p>
<h2 id="methods">Methods</h2>
<p>__pre_compute : prepare input data
run
: do compute matmul
out
: return output result</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/blas.py#L194-L345" class="git-link">Browse git</a>
</summary>
<pre><code class="python">class Vmul():
    &#34;&#34;&#34; define a Matmul object, release the function .

    Attributes::
        context: the output image bind with an AscendArray object, image.shape(tupe(h, w, c))
        mat_a  : 
        mat_b  : 
        mat_c  :

    Methods:
        __pre_compute : prepare input data
        run           : do compute matmul
        out           : return output result
    &#34;&#34;&#34;
    def __init__(self, context, mat_a, vec_x, vec_y, alpha=1.0, beta=0.0, highprec=True):
        if not isinstance(context, int):
            raise TypeError(f&#34;Input context expects an int, but got {type(context)}.&#34;)

        if not isinstance(mat_a, AscendArray):
            raise TypeError(f&#34;Input mat_a expects an AscendArray, but got {type(mat_a)}.&#34;)

        if not isinstance(vec_x, AscendArray):
            raise TypeError(f&#34;Input vec_x expects an AscendArray, but got {type(vec_x)}.&#34;)

        if not isinstance(vec_y, AscendArray):
            raise TypeError(f&#34;Input vec_y expects an AscendArray, but got {type(vec_y)}.&#34;)

        assert mat_a.format == vec_x.format, f&#34;Input mat_a and vec_x expects same format.&#34;

        # assign self value
        self.context = context
        self.stream = create_stream(context)
        self.highprec = 1 if highprec else 0

        # set op model dir
        ret = acl.op.set_model_dir(&#34;op_models&#34;)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;op set model dir failed, return {ret}.&#34;)
        
        # calculate m, n, k and trans alpha and beta to np.ndarray
        self.__pre_compute(mat_a, vec_x, vec_y, alpha, beta)

        # do blas gemm_ex and synchronize stream
        self.run()

        # free input data memory
        self.free_in()


    def __pre_compute(self, mat_a, vec_x, vec_y, alpha, beta):
        &#34;&#34;&#34; calculate m, n, k and copy alpha/beta to device.
        Args:
            mat_a : (AscendArray) matrix A
            vec_x : (AscendArray) vector x
            vec_y : (AscendArray) vector y
            alpha : (float value)
            beta  : (float value)

        Returns:
            None
        &#34;&#34;&#34;
        if mat_a.format in [
            PIXEL_FORMAT_YUV_SEMIPLANAR_420,
            PIXEL_FORMAT_YVU_SEMIPLANAR_420
            ]:
            self.m = mat_a.shape[-1]
            self.n = mat_a.shape[0] * 2 // 3
            self.k = mat_b.shape[1] * 2 // 3
        elif mat_a.format in [
            PIXEL_FORMAT_YUV_SEMIPLANAR_422,
            PIXEL_FORMAT_YVU_SEMIPLANAR_422
            ]:
            self.m = mat_a.shape[-1]
            self.n = mat_a.shape[0] * 2 // 3
            self.k = mat_b.shape[1] * 2 // 3
        elif mat_a.format == &#39;NCHW&#39;:
            self.m = mat_a.shape[-1]
            self.n = mat_a.shape[0]
            self.k = mat_b.shape[1]
        elif mat_a.format == &#39;NHWC&#39;:
            self.m = mat_a.shape[-1]
            self.n = mat_a.shape[0]
            self.k = mat_b.shape[1]
        else:
            raise ValueError(f&#34;Input data format not support.&#34;)

        alpha = np.array([alpha]).astype(mat_a.dtype)
        beta  = np.array([beta]).astype(mat_a.dtype)
        self.alpha = AscendArray.clone(alpha)
        self.beta = AscendArray.clone(beta)

    def run(self):
        &#34;&#34;&#34; run op.
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        a_type = type_map(self.mat_a.dtype)
        x_type = type_map(self.vec_x.dtype)
        y_type = type_map(self.vec_y.dtype)
        # do vmul asyncronize
        ret = acl.blas.gemv_ex(ACL_TRANS_N, 
                                self.m, self.n,
                                self.alpha.ascend_data, 
                                self.mat_a.ascend_data, self.k, a_type, 
                                self.vec_x.ascend_data, incx, x_type, 
                                self.beta.ascend_data, 
                                self.vec_y.ascend_data, incy, y_type, 
                                self.high_prec,
                                self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to do blas gemv_ex, return {ret}.&#34;)

        # do synchronize stream 
        ret = acl.rt.synchronize_stream(self.stream)
        if ret != ACL_SUCCESS:
            raise ValueError(f&#34;Failed to synchronize stream in running blas gemv_ex, return {ret}.&#34;)
    
    @property
    def out(self):
        &#34;&#34;&#34; run op.
        Args:
            None

        Returns:
            vec_y
        &#34;&#34;&#34;
        return self.vec_y

    def _free_ab(self):
        &#34;&#34;&#34; free alpha and beta data memory
        Args:
            None

        Returns:
            None
        &#34;&#34;&#34;
        if hasattr(self, &#39;alpha&#39;):
            del self.alpha

        if hasattr(self, &#39;beta&#39;):
            del self.beta

    def __del__(self):
        if hasattr(self, &#39;stream&#39;):
            ret = acl.rt.destroy_stream(self.stream)
            assert ret == ACL_SUCCESS, f&#34;destroy stream failed, return {ret}.&#34;
        
        # free alpha and beta data memory
        self._free_ab()</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="ascend.ops.Vmul.out"><code class="name">var <span class="ident">out</span></code></dt>
<dd>
<div class="desc"><p>run op.</p>
<h2 id="args">Args</h2>
<p>None</p>
<h2 id="returns">Returns</h2>
<p>vec_y</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/blas.py#L314-L323" class="git-link">Browse git</a>
</summary>
<pre><code class="python">@property
def out(self):
    &#34;&#34;&#34; run op.
    Args:
        None

    Returns:
        vec_y
    &#34;&#34;&#34;
    return self.vec_y</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="ascend.ops.Vmul.run"><code class="name flex">
<span>def <span class="ident">run</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>run op.</p>
<h2 id="args">Args</h2>
<p>None</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
<a href="https://github.com/pdoc3/pdoc/blob/c0c4662b8bfdd479b00c923799cfbd69bbea0eb8/ascend/ops/blas.py#L285-L312" class="git-link">Browse git</a>
</summary>
<pre><code class="python">def run(self):
    &#34;&#34;&#34; run op.
    Args:
        None

    Returns:
        None
    &#34;&#34;&#34;
    a_type = type_map(self.mat_a.dtype)
    x_type = type_map(self.vec_x.dtype)
    y_type = type_map(self.vec_y.dtype)
    # do vmul asyncronize
    ret = acl.blas.gemv_ex(ACL_TRANS_N, 
                            self.m, self.n,
                            self.alpha.ascend_data, 
                            self.mat_a.ascend_data, self.k, a_type, 
                            self.vec_x.ascend_data, incx, x_type, 
                            self.beta.ascend_data, 
                            self.vec_y.ascend_data, incy, y_type, 
                            self.high_prec,
                            self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to do blas gemv_ex, return {ret}.&#34;)

    # do synchronize stream 
    ret = acl.rt.synchronize_stream(self.stream)
    if ret != ACL_SUCCESS:
        raise ValueError(f&#34;Failed to synchronize stream in running blas gemv_ex, return {ret}.&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<header>
<a class="homelink" rel="home" title="ascendfly Home" href="https://gitee.com/ascend-fae/ascendfly">
<img src="https://gitee.com/ascend-fae/ascendfly/blob/master/doc/logo/logo.png" alt=""> ascendfly
</a>
</header>
<div class="gcse-search" style="height: 70px"
data-as_oq="site:pdoc3.github.io inurl:github.com/pdoc3"
data-gaCategoryParameter="ascend.ops">
</div>
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="ascend" href="../index.html">ascend</a></code></li>
</ul>
</li>
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="ascend.ops.blas" href="blas.html">ascend.ops.blas</a></code></li>
<li><code><a title="ascend.ops.csc" href="csc.html">ascend.ops.csc</a></code></li>
<li><code><a title="ascend.ops.math" href="math.html">ascend.ops.math</a></code></li>
<li><code><a title="ascend.ops.op" href="op.html">ascend.ops.op</a></code></li>
<li><code><a title="ascend.ops.util" href="util.html">ascend.ops.util</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ascend.ops.ArgMax" href="#ascend.ops.ArgMax">ArgMax</a></code></h4>
<ul class="">
<li><code><a title="ascend.ops.ArgMax.data" href="#ascend.ops.ArgMax.data">data</a></code></li>
<li><code><a title="ascend.ops.ArgMax.run" href="#ascend.ops.ArgMax.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ascend.ops.Cast" href="#ascend.ops.Cast">Cast</a></code></h4>
<ul class="">
<li><code><a title="ascend.ops.Cast.data" href="#ascend.ops.Cast.data">data</a></code></li>
<li><code><a title="ascend.ops.Cast.run" href="#ascend.ops.Cast.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ascend.ops.Matmul" href="#ascend.ops.Matmul">Matmul</a></code></h4>
<ul class="">
<li><code><a title="ascend.ops.Matmul.free_ab" href="#ascend.ops.Matmul.free_ab">free_ab</a></code></li>
<li><code><a title="ascend.ops.Matmul.out" href="#ascend.ops.Matmul.out">out</a></code></li>
<li><code><a title="ascend.ops.Matmul.run" href="#ascend.ops.Matmul.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ascend.ops.Permute" href="#ascend.ops.Permute">Permute</a></code></h4>
<ul class="">
<li><code><a title="ascend.ops.Permute.data" href="#ascend.ops.Permute.data">data</a></code></li>
<li><code><a title="ascend.ops.Permute.run" href="#ascend.ops.Permute.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ascend.ops.Transpose" href="#ascend.ops.Transpose">Transpose</a></code></h4>
<ul class="">
<li><code><a title="ascend.ops.Transpose.data" href="#ascend.ops.Transpose.data">data</a></code></li>
<li><code><a title="ascend.ops.Transpose.run" href="#ascend.ops.Transpose.run">run</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="ascend.ops.Vmul" href="#ascend.ops.Vmul">Vmul</a></code></h4>
<ul class="">
<li><code><a title="ascend.ops.Vmul.out" href="#ascend.ops.Vmul.out">out</a></code></li>
<li><code><a title="ascend.ops.Vmul.run" href="#ascend.ops.Vmul.run">run</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p><span style="color:#ddd">&#21328;</span></p>
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>